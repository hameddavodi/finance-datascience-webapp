{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9349181-1e80-41da-9b96-018f506311bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from binance.client import Client\n",
    "import numpy as np\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy.orm import Session,declarative_base, sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String\n",
    "import os\n",
    "\n",
    "\n",
    "# Replace with your Binance API key and secret\n",
    "api_key = 'qHsgWDMVoKKB9CNacVKPc9giyRyVgm8YdGqAFjJTibzc6FEMKEJdmBXvDlVzrTiK'\n",
    "api_secret = 'i45SJIRjGM9SL4fe42hC7V7c9B6YUUiQm7lVELHTZGzrE50K25pfv8ZagnuWEduI'\n",
    "\n",
    "client = Client( api_key = api_key , api_secret = api_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab753f1b-97d7-4f94-a61e-81211e3cfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define connections setup and write to database\n",
    "########################################################################################################\n",
    "# Here I defined a funtion to connect to the Server\n",
    "def connect_to_database():\n",
    "    # Make these availbale anywhere esle\n",
    "    global engine, session\n",
    "    \n",
    "    # Connect to your PostgreSQL database\n",
    "    # Here I defined parameters to have a clean structure\n",
    "    params = dict(\n",
    "        database=\"wisdomise\",\n",
    "        user=\"henry\",\n",
    "        password=\"henry\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        )\n",
    "    # Here I defined the connection like using f\"{}\" in order to pass parameters with .fomart(**params)\n",
    "    engine_string = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**params)\n",
    "\n",
    "    # Create Engine instance\n",
    "    engine = create_engine(engine_string,echo =False)\n",
    "    # And Create Session with engine\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    return engine\n",
    "\n",
    "########################################################################################################\n",
    "# Here I defined a function to write to the table with a given query string\n",
    "def write_to_database_historical(query):\n",
    "\n",
    "    try:\n",
    "        # Create a session object to interact with the database\n",
    "        session.execute(query)\n",
    "        session.commit()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to the database: {str(e)}\")\n",
    "########################################################################################################\n",
    "# Here I defined a function to close the connection \n",
    "def close_database_connection():\n",
    "    \n",
    "    # Close the session and the engine\n",
    "    session.close()\n",
    "    engine.dispose()\n",
    "\n",
    "########################################################################################################\n",
    "# Initialize the connection \n",
    "# Call connect_to_database to initialize engine and session\n",
    "engine = connect_to_database() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649f822a-b632-4425-b72f-72fdc42ccdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql+psycopg2://henry:***@localhost:5432/wisdomise)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622de0e0-b759-4ec3-b293-8c02ba23ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Here I create a class to create a table within the database \n",
    "# Note that Name of the class has nothing to do with table name since I put the name in __tablename__\n",
    "# Note that I defined __init___ and __repr___ functions in order to access to the database object created by the engine\n",
    "\n",
    "Base = declarative_base()\n",
    "class DataBase(Base):\n",
    "\n",
    "    __tablename__ = \"final_data\"\n",
    "    # Now we can easily define the schema of the table and set all primary, foreign and also types\n",
    "    Time = Column(\"Time\", String(50), primary_key=True)\n",
    "    Price = Column(\"Price\", String(50))\n",
    "\n",
    "    def __init__(self,Time, Price):\n",
    "        self.Time = Time\n",
    "        self.Price = Price\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.Time})({self.Price})\"\n",
    "########################################################################################################\n",
    "# Now to create the database we should call the class DataBase\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "# There are some further notes to collect from the youtube you have bookmarket\n",
    "\n",
    "\n",
    "# This function should be called in order to make dataframe table\n",
    "try:\n",
    "    Base.metadata.create_all(engine)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "050f0c9b-4202-4428-868e-f4930653c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Define realtime data stream setup using binance python lib\n",
    "def get_last_closed_price(symbol, interval):\n",
    "\n",
    "    # First, it is better to create a mapping variable in order to collect right data.\n",
    "    # Define the mapping of intervals to Binance API constants\n",
    "    interval_mapping = {\n",
    "        '1m': Client.KLINE_INTERVAL_1MINUTE,\n",
    "        '1h': Client.KLINE_INTERVAL_1HOUR,\n",
    "        '1d': Client.KLINE_INTERVAL_1DAY,\n",
    "        '1w': Client.KLINE_INTERVAL_1WEEK,\n",
    "    }\n",
    "    \n",
    "    # Retrieve the Kline data for the specified symbol and interval\n",
    "    klines_realtime = client.get_klines(symbol=symbol, interval=interval_mapping[interval])\n",
    "    # Here I can turn it to the numpy array\n",
    "    # array = np.array(klines_realtime)\n",
    "    # time_stamp = array[-2, 0]\n",
    "    # Price = array[-2,4]\n",
    "    # frame = np.column_stack((time_stamp, Price))\n",
    "\n",
    "    # UPDATE\n",
    "    # I decided to use pandas instead of the numpy and I use all the inputs instead of selecting specific column\n",
    "    Kline_realtime_data = pd.DataFrame(klines_realtime).iloc[-1:,4]\n",
    "    # Now the line above will return a single vector like (row) of the last data \n",
    "    \n",
    "    return Kline_realtime_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c47c19d0-5225-4d8e-90b8-3fa8a40f0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Now let's define historical data query\n",
    "# Start and end format should be like --> \"1 Nov, 2023\"\n",
    "# Most of the parts are same as before\n",
    "\n",
    "def get_historical_data(symbol,interval, start, end):\n",
    "    \n",
    "    array = np.zeros((5,2))\n",
    "    interval_mapping = {\n",
    "        '1m': Client.KLINE_INTERVAL_1MINUTE,\n",
    "        '1h': Client.KLINE_INTERVAL_1HOUR,\n",
    "        '1d': Client.KLINE_INTERVAL_1DAY,\n",
    "        '1w': Client.KLINE_INTERVAL_1WEEK,\n",
    "    }\n",
    "\n",
    "    historical_data = client.get_historical_klines(symbol, interval_mapping[interval], start, end)\n",
    "    \n",
    "    data_frame = pd.DataFrame(historical_data, columns = [   \n",
    "    \"KlineOpen\",\n",
    "    \"OpenPrice\",\n",
    "    \"HighPrice\",\n",
    "    \"LowPrice\",\n",
    "    \"ClosePrice\",\n",
    "    \"Volume\",\n",
    "    \"KlineClose\",\n",
    "    \"QuoteVolume\",\n",
    "    \"NumTrades\",\n",
    "    \"TakerBuyBase\",\n",
    "    \"TakerBuyQuote\",\n",
    "    \"id\"\n",
    "\n",
    "    ])\n",
    "    # Here I am defind Coin name and TimeFrame Column\n",
    "    data_frame.insert(0, \"Coin\", symbol)\n",
    "    data_frame.insert(1, \"TimeFrame\", interval_mapping[interval])\n",
    "    \n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31d3ba2-a941-43a4-8731-d158fdae3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Define engine to get historical data and store it in CSVs\n",
    "\n",
    "def write(sym, inter, start,end):\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    \n",
    "    for i,j in enumerate(sym):\n",
    "        \n",
    "        for m in inter:\n",
    "            \n",
    "            temp = get_historical_data(j , m , start, end)\n",
    "            \n",
    "            # Be careful About the path below since we hard coded it\n",
    "            #temp.to_csv(f'/Users/hamed/Downloads/data/{j}_{m}.csv', index = True)\n",
    "\n",
    "\n",
    "            concatenated_df = pd.concat([concatenated_df, temp], axis=0)\n",
    "            concatenated_df = concatenated_df.rename(columns={'Unnamed: 0' : 'key'})\n",
    "            concatenated_df['key'] = range(1, len(concatenated_df) + 1)\n",
    "            \n",
    "    concatenated_df = concatenated_df.set_index('key')\n",
    "            \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7479c2-a7dd-4efc-895b-492ba4e4b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Get the list of top 10 Coins with highest marketcap and returns\n",
    "def get_products(start, end):\n",
    "    \n",
    "    array = np.zeros((5,2))\n",
    "    coins_list = []\n",
    "    market_cap = []    \n",
    "    price = []\n",
    "    returns = []\n",
    "    data = client.get_ticker()\n",
    "\n",
    "    for coin in data:\n",
    "        f = coin['symbol']\n",
    "        if f.endswith('USDT'):\n",
    "            \n",
    "            coins_list.append(coin['symbol'])\n",
    "            \n",
    "            pr = float(coin['lastPrice'])\n",
    "            \n",
    "            volume = float(coin['volume'])\n",
    "            \n",
    "            mc = pr * volume\n",
    "            \n",
    "            price.append(pr)\n",
    "            market_cap.append(mc)\n",
    "            \n",
    "    coins = list(pd.DataFrame({\"Coin\": coins_list,\"Price\": price, \"MarketCap\": market_cap}).sort_values(by = [\"MarketCap\"], ascending = False)[\"Coin\"].head(10))\n",
    "   \n",
    "    #for coin in list(market_frame[\"Coin\"]):\n",
    "            \n",
    "    #    data = get_historical_data(coin,\"1d\", start, end)[[\"ClosePrice\"]]\n",
    "        \n",
    "    #    ret = np.mean(np.log(data.astype(float).pct_change().dropna()+1))\n",
    "        \n",
    "    #    returns.append([coin,ret])\n",
    "\n",
    "    #   coins = list(pd.DataFrame(returns, columns = [\"Coin\",\"1_day_return\"]).sort_values(by = \"1_day_return\", ascending = False).head(10)[\"Coin\"])\n",
    "    \n",
    "    \n",
    "    return coins\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51808f96-554b-4589-aae5-8100bcc319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Here is the function to write the CSV to Postgres\n",
    "def create_sql_table_from_csv(engine):\n",
    "    # Again hard coded path\n",
    "    directory_path = \"/Users/hamed/Downloads/data/\"\n",
    "    # Get all files in a directory\n",
    "    files = os.listdir(directory_path)\n",
    "    csv = []\n",
    "    # Check for the CSV files type\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv.append(file)\n",
    "    # Create empty Datafrane\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    # read all CSV files and concatenate (horizontaly merge) the files\n",
    "    for file in csv:\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"/Users/hamed/Downloads/data/{file}\")\n",
    "            \n",
    "            concatenated_df = pd.concat([concatenated_df, df], axis=0)\n",
    "            concatenated_df = concatenated_df.rename(columns={'Unnamed: 0' : 'key'})\n",
    "            concatenated_df['key'] = range(1, len(concatenated_df) + 1)\n",
    "            concatenated_df = concatenated_df.set_index('key')\n",
    "    \n",
    "    # Create a SQLAlchemy table object\n",
    "    metadata = MetaData()\n",
    "    table = Table('dataframe', metadata)\n",
    "    \n",
    "    # Define columns based on the DataFrame headers\n",
    "    for column_name in concatenated_df.columns:\n",
    "        \n",
    "        # Here, I specified the datatypes are String50 due to the fact that all retrived data from binance python has this datatype\n",
    "        column = Column(column_name, String(50))\n",
    "        table.append_column(column)\n",
    "    \n",
    "     # Create the table in the database\n",
    "    metadata.create_all(engine)\n",
    "    \n",
    "    concatenated_df.to_sql('dataframe', con=engine, if_exists='replace', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acce6f93-fea5-4b8a-9a10-5bb233df02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Finally, here is the function to write the data into PostgreSQL DIRECTLY\n",
    "def write_sql_table(start,end):\n",
    "\n",
    "    inter = ['1d', '1w']\n",
    "    \n",
    "    engine = connect_to_database()\n",
    "    \n",
    "    symbol = ['BTCUSDT','ETHUSDT','XRPUSDT','SOLUSDT','ADAUSDT','LINKUSDT','MATICUSDT','DOTUSDT','AVAXUSDT','ATOMUSDT']\n",
    "    symbol.sort()\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    \n",
    "    for j in symbol:\n",
    "        \n",
    "        for m in inter:\n",
    "            \n",
    "            temp = get_historical_data(j , m , start, end)\n",
    "\n",
    "            concatenated_df = pd.concat([concatenated_df, temp], axis=0)\n",
    "            concatenated_df = concatenated_df.rename(columns={'Unnamed: 0' : 'key'})\n",
    "            concatenated_df['key'] = range(1, len(concatenated_df) + 1)\n",
    "            \n",
    "    concatenated_df = concatenated_df.set_index('key')\n",
    "            \n",
    "    metadata = MetaData()\n",
    "    table = Table('dataframe', metadata)\n",
    "    \n",
    "    # Define columns based on the DataFrame headers\n",
    "    for column_name in concatenated_df.columns:\n",
    "        \n",
    "        # Here, I specified the datatypes are String50 due to the fact that all retrived data from binance python has this datatype\n",
    "        column = Column(column_name, String(50))\n",
    "        table.append_column(column)\n",
    "    \n",
    "     # Create the table in the database\n",
    "    metadata.create_all(engine)\n",
    "    \n",
    "    concatenated_df.to_sql('dataframe', con=engine, if_exists='replace', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4b3073-db1d-499e-9572-08858e9b0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "########################################################################################################\n",
    "# Lets test the functions\n",
    "\n",
    "start = \"2023-01-01\"\n",
    "end = \"2023-11-01\"\n",
    "write_sql_table(start,end)\n",
    "#create_sql_table_from_csv(start,end)\n",
    "########################################################################################################\n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d174b2c-c8d0-4c27-8140-3a0e293d6178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTCUSDT',\n",
       " 'ETHUSDT',\n",
       " 'USDCUSDT',\n",
       " 'SOLUSDT',\n",
       " 'XRPUSDT',\n",
       " 'GASUSDT',\n",
       " 'BNBUSDT',\n",
       " 'ORDIUSDT',\n",
       " 'FTTUSDT',\n",
       " 'FDUSDUSDT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_products(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eba1d05f-fe40-4477-9691-9c25a663b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "symbol = ['BTCUSDT','ETHUSDT','XRPUSDT','SOLUSDT','ADAUSDT','LINKUSDT','MATICUSDT','DOTUSDT','AVAXUSDT','ATOMUSDT']\n",
    "symbol.sort()\n",
    "\n",
    "def eval_price(symbol, start_date):\n",
    "    end_date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    con_real = []\n",
    "    for j in symbol:\n",
    "        temp_real = np.array((get_last_closed_price(j , '1d')))\n",
    "        con_real.append(temp_real)\n",
    "\n",
    "\n",
    "    con_date = pd.DataFrame()\n",
    "    for j in symbol:\n",
    "        temp_date = get_historical_data(j , '1d' , start_date, end_date)\n",
    "        con_date = pd.concat([con_date, temp_date], axis=0)\n",
    "    con_date = np.array(con_date[\"ClosePrice\"][con_date[\"KlineOpen\"] ==  int(datetime.strptime(start_date, '%Y-%m-%d').replace(tzinfo=timezone.utc).timestamp()) * 1000 ] ).astype(float)           \n",
    "    return np.array(con_real).astype(float).flatten(), con_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ea7d0b6e-11e3-42c3-af83-047daf33949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2023-11-02'\n",
    "x,y = eval_price(symbol,start_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
